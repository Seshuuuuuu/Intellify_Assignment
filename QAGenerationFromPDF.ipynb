{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Form, Request, Response, File, Depends, HTTPException, status\n",
        "from fastapi.responses import RedirectResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.encoders import jsonable_encoder\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import QAGenerationChain\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import uvicorn\n",
        "import aiofiles\n",
        "from PyPDF2 import PdfReader\n",
        "import csv"
      ],
      "metadata": {
        "id": "jlosIwJJd1SR"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI()\n",
        "\n",
        "\n",
        "templates = Jinja2Templates(directory=\"templates\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "qBWuXn3qeZss"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set file path\n",
        "# file_path = '/content/Big Mac Index.pdf'\n",
        "\n",
        "def count_pdf_pages(pdf_path):\n",
        "    try:\n",
        "        pdf = PdfReader(pdf_path)\n",
        "        return len(pdf.pages)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "OCWM4VTkfFib"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_processing(file_path):\n",
        "\n",
        "    # Load data from PDF\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    data = loader.load()\n",
        "\n",
        "    question_gen = ''\n",
        "\n",
        "    for page in data:\n",
        "        question_gen += page.page_content\n",
        "\n",
        "    splitter_ques_gen = TokenTextSplitter(\n",
        "        model_name = 'gpt-3.5-turbo',\n",
        "        chunk_size = 10000,\n",
        "        chunk_overlap = 200\n",
        "    )\n",
        "\n",
        "    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)\n",
        "\n",
        "    document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]\n",
        "\n",
        "    splitter_ans_gen = TokenTextSplitter(\n",
        "        model_name = 'gpt-3.5-turbo',\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap = 100\n",
        "    )\n",
        "\n",
        "\n",
        "    document_answer_gen = splitter_ans_gen.split_documents(\n",
        "        document_ques_gen\n",
        "    )\n",
        "\n",
        "    return document_ques_gen, document_answer_gen"
      ],
      "metadata": {
        "id": "HVWOQlhwefyN"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_pipeline(file_path):\n",
        "\n",
        "    document_ques_gen, document_answer_gen = file_processing(file_path)\n",
        "\n",
        "    llm_ques_gen_pipeline = ChatOpenAI(\n",
        "        temperature = 0.3,\n",
        "        model = \"gpt-3.5-turbo\"\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    You are an expert at creating questions based on materials and documentation.\n",
        "    Your goal is to prepare True or False questions for their exam tests.\n",
        "    You do this by asking questions about the text below:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Create True or False type questions that will prepare the students for their tests.\n",
        "    Make sure not to lose any important information.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "    refine_template = (\"\"\"\n",
        "    You are an expert at creating practice True or False questions based on material and documentation.\n",
        "    Your goal is to help a learner prepare for a test.\n",
        "    We have received some practice True or False questions to a certain extent: {existing_answer}.\n",
        "    We have the option to refine the existing questions or add new ones.\n",
        "    (only if necessary) with some more context below.\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Given the new context, refine the original questions in English.\n",
        "    If the context is not helpful, please provide the original questions.\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
        "        input_variables=[\"existing_answer\", \"text\"],\n",
        "        template=refine_template,\n",
        "    )\n",
        "\n",
        "    ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline,\n",
        "                                            chain_type = \"refine\",\n",
        "                                            verbose = True,\n",
        "                                            question_prompt=PROMPT_QUESTIONS,\n",
        "                                            refine_prompt=REFINE_PROMPT_QUESTIONS)\n",
        "\n",
        "    ques = ques_gen_chain.run(document_ques_gen)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    vector_store = FAISS.from_documents(document_answer_gen, embeddings)\n",
        "\n",
        "    llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    ques_list = ques.split(\"\\n\")\n",
        "    filtered_ques_list = [element for element in ques_list if element.endswith('?') or element.endswith('.')]\n",
        "\n",
        "    answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen,\n",
        "                                                chain_type=\"stuff\",\n",
        "                                                retriever=vector_store.as_retriever())\n",
        "\n",
        "    return answer_generation_chain, filtered_ques_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Answer each question and save to a file\n",
        "# for question in question_list:\n",
        "#     print(\"Question: \", question)\n",
        "#     answer = answer_gen_chain.run(question)\n",
        "#     print(\"Answer: \", answer)\n",
        "#     print(\"--------------------------------------------------\\\\n\\\\n\")\n",
        "#     # Save answer to file\n",
        "#     with open(\"answers.txt\", \"a\") as f:\n",
        "#         f.write(\"Question: \" + question + \"\\\\n\")\n",
        "#         f.write(\"Answer: \" + answer + \"\\\\n\")\n",
        "#         f.write(\"--------------------------------------------------\\\\n\\\\n\")\n"
      ],
      "metadata": {
        "id": "nZ31XVPDfaqO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_csv (file_path):\n",
        "    answer_generation_chain, ques_list = llm_pipeline(file_path)\n",
        "    base_folder = 'static/output/'\n",
        "    if not os.path.isdir(base_folder):\n",
        "        os.mkdir(base_folder)\n",
        "    output_file = base_folder+\"QA.csv\"\n",
        "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerow([\"Question\", \"Answer\"])  # Writing the header row\n",
        "\n",
        "        for question in ques_list:\n",
        "            print(\"Question: \", question)\n",
        "            answer = answer_generation_chain.run(question)\n",
        "            print(\"Answer: \", answer)\n",
        "            print(\"--------------------------------------------------\\n\\n\")\n",
        "\n",
        "            # Save answer to CSV file\n",
        "            csv_writer.writerow([question, answer])\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "ckIrMiIogmKC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/\")\n",
        "async def index(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request})"
      ],
      "metadata": {
        "id": "LlUu838ahBsI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/upload\")\n",
        "async def chat(request: Request, pdf_file: bytes = File(), filename: str = Form(...)):\n",
        "    base_folder = 'static/docs/'\n",
        "    if not os.path.isdir(base_folder):\n",
        "        os.mkdir(base_folder)\n",
        "    pdf_filename = os.path.join(base_folder,  'Big Mac Index.pdf')\n",
        "\n",
        "    async with aiofiles.open(pdf_filename, 'wb') as f:\n",
        "        await f.write(pdf_file)\n",
        "    # page_count = count_pdf_pages(pdf_filename)\n",
        "    # if page_count > 5:\n",
        "    #     return Response(jsonable_encoder(json.dumps({\"msg\": 'error'})))\n",
        "    response_data = jsonable_encoder(json.dumps({\"msg\": 'success',\"pdf_filename\": pdf_filename}))\n",
        "    res = Response(response_data)\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "vHKBbVZfhF55"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Res)"
      ],
      "metadata": {
        "id": "x1-X5GeprvFH",
        "outputId": "72f9ba5b-3db7-46dd-fd31-329470f9cf25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 25 tricky true or false questions based on the text, along with answers:\n",
            "\n",
            "1. True or False: The Big Mac Index was introduced in The Economist by Pam Woodall in 1986.\n",
            "   - True\n",
            "\n",
            "2. True or False: The Big Mac Index is primarily used for measuring the quality of fast-food burgers.\n",
            "   - False\n",
            "\n",
            "3. True or False: The Big Mac Index compares the relative prices of purchasing a Big Mac in different countries.\n",
            "   - True\n",
            "\n",
            "4. True or False: The Big Mac Index was initially intended to be a serious tool for evaluating exchange rates.\n",
            "   - False\n",
            "\n",
            "5. True or False: The concept of the Big Mac Index is based on the idea of purchasing power parity.\n",
            "   - True\n",
            "\n",
            "6. True or False: According to the text, over 3,000 consumer goods and services are included in the current PPP calculations.\n",
            "   - True\n",
            "\n",
            "7. True or False: The price of a Big Mac is derived solely from the cost of its ingredients.\n",
            "   - False\n",
            "\n",
            "8. True or False: The purpose of the Big Mac Index is to calculate the exact exchange rate between two currencies.\n",
            "   - False\n",
            "\n",
            "9. True or False: The base country used in the Big Mac Index is typically the United Kingdom.\n",
            "   - False\n",
            "\n",
            "10. True or False: The Big Mac Index can be used to analyze whether a currency is overvalued or undervalued.\n",
            "    - True\n",
            "\n",
            "11. True or False: The Big Mac Index compares the actual exchange rate with the purchasing power parity exchange rate.\n",
            "    - True\n",
            "\n",
            "12. True or False: The Big Mac Index was introduced as a serious economic tool and has remained unchanged since its inception.\n",
            "    - False\n",
            "\n",
            "13. True or False: In 2004, The Economist introduced a variation of the Big Mac Index called the Tall Latte Index.\n",
            "    - True\n",
            "\n",
            "14. True or False: The iPod Index is based on the idea that the value of iPods should be consistent globally.\n",
            "    - True\n",
            "\n",
            "15. True or False: The Gold-Mac-Index measures the purchasing power of gold relative to the price of a Big Mac.\n",
            "    - True\n",
            "\n",
            "16. True or False: The Big Mac Index has no limitations and provides an accurate measurement of purchasing power parity.\n",
            "    - False\n",
            "\n",
            "17. True or False: The Big Mac Index is limited by geographical coverage due to the presence of KFC franchises.\n",
            "    - False\n",
            "\n",
            "18. True or False: The price of a Big Mac in a country is always directly proportional to the country's GDP.\n",
            "    - False\n",
            "\n",
            "19. True or False: In some markets, a high-volume and low-margin approach to pricing Big Macs is preferred.\n",
            "    - True\n",
            "\n",
            "20. True or False: The Big Mac Index can be manipulated by governments to influence their country's performance.\n",
            "    - True\n",
            "\n",
            "21. True or False: The nutritional value of Big Macs is consistent across all countries.\n",
            "    - False\n",
            "\n",
            "22. True or False: Beef burgers are available at all McDonald's outlets worldwide, including India.\n",
            "    - False\n",
            "\n",
            "23. True or False: The price of a Big Mac in Iceland increased by 20% before the McDonald's outlets closed.\n",
            "    - True\n",
            "\n",
            "24. True or False: Hong Kong had the fastest average working time required to buy one Big Mac in 2015.\n",
            "    - True\n",
            "\n",
            "25. True or False: The working time required to buy one Big Mac in Nairobi, Kenya, was less than one hour in 2015.\n",
            "    - False\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}